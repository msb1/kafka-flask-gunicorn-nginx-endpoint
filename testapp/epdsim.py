'''
Created on Feb 11, 2019

@author: Barnwaldo

-- Below script generates simulated analytics data for sending to Kafka, Kafka Connect and Kafka Streams applications
-- This is a Flask application with a generic bootstrap web page 
-- The application connects to javascript through socket.io using flask-socketio
-- A Kafka Producer and Consumer are started using kafka-python in separate threads 
-- The test data is sent as a json string with the producer to a topic and then received/consumed with the consumer
-- The sent/received messages are displayed in separate cards on the web page
-- Start/stop for the threads is performed with menu buttons on left menu bar of the web page
'''
import numpy as np
import random
import json
import atexit
import gevent
import signal
from gevent import Greenlet
from gevent import monkey
from flask_socketio import SocketIO
from flask import Flask, render_template
# from kafka import KafkaProducer, KafkaConsumer
from confluent_kafka import Consumer, Producer, KafkaError, KafkaException
from builtins import AttributeError

monkey.patch_socket()
app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret!!!really()secret##@@xx'
# app.config['DEBUG'] = True

producer_total = 0
consumer_total = 0
producer_topic = "endpt00"
consumer_topics = ["endpt00"]
consumer_timeout = 1000

socketio = SocketIO(app)
gthreads = {}

config = {'bootstrap.servers': '192.168.5.3:9092', 'group.id': 'epdsim', 'session.timeout.ms': 6000}

'''
define simulator with sensors and category parameter and number of classes (results)
see sample epd.conf file

(1) bootstrap_servers is set to kafka broker IP address (and port)
(2) consumer_timeout is set for kafka consumer
(3) categorics include category parameters -- each cat parameter has a high range and a low range list; the number of 
    list entries equals the number of levels. The list values are chosen so that the level is generated for a uniform
    random variable [0, 1]
(4) sensors are chosen where the list is determined as follows:
    First sublist entry (case)
        0 = two means correlated with n_class (normal distributions)
        1 = two means anti-correlated with n_class (normal distributions)
        2 = one mean -- no correlation (normal distribution)
        3 = uniformly distributed [0, 1] with no correlation
    Second and third sublist entry --> means in interval [0, 1] corresponding to 0 and 1 in first entry
    Fourth and fifth sublist entry --> standard devs in interval [0, 1] corresponding to 0 and 1 in first entry
    For first sublist entry = 2, use only second and fourth sublist entries (others set to zero)

    The number after the list is a scale factor since the sensor is generated by either a normal rv or uniform rv according to above.
    The scale factor multiplies the rv for the output
(5) topic is the kafka broker topic for the producer and consumer
(6) success_rate is fraction nclass=1
(7) cat_error_rate is fraction categories do not coincide with class level

'''


def readParams():
#     with open('/home/barnwaldo/epd.conf') as json_file:  
#         return json.load(json_file)
    with open('epd.conf') as json_file:  
        return json.load(json_file)


def makeRecord(par):
    '''
    Method to create simulated analytics data records to be sent with Kafka Producer thread
    '''
    
    # generate numerics (sensor) simulated data
    jsonDict = {}
    y = 1 if np.random.random_sample() < par['success'] else 0 
    jsonDict['class'] = y
    # jsonDict['time'] = str(datetime.datetime.now())
    # jsonDict['endpoint'] = par['topic']
    sensors = par['sensors']
    for key, value in sensors.items():
        # unpack sensor info
        sensor_defs = value[0]
        scale = value[1]
        case = sensor_defs[0]
        # determine simulated sensor output
        result = 0
        if case == 0:
            if y == 1:
                result =  np.random.normal(sensor_defs[2], sensor_defs[4])
            else:
                result =  np.random.normal(sensor_defs[1], sensor_defs[3])
        elif case == 1:
            if y == 1:
                result =  np.random.normal(sensor_defs[1], sensor_defs[3])
            else:
                result =  np.random.normal(sensor_defs[2], sensor_defs[4])
        elif case == 2:
            result =  np.random.normal(sensor_defs[1], sensor_defs[3])
        elif case == 3:
            result =  np.random.random_sample()
        jsonDict[key] = round(scale * result, 3)

    # generate categorics simulated data
    categorics = par['categorics']
    for key, value in categorics.items():
        range_hi = value[0]
        range_lo = value[1]
        labels = value[2]
        nlevels = len(labels)
    
        result = 0
        rnd = np.random.random_sample()
        if y == 1:
            for j in range(nlevels):
                if(rnd < range_hi[j]):
                    result = j
                    break
        else:
            for j in range(nlevels):
                if(rnd < range_lo[j]):
                    result = j
                    break
        
        if np.random.random_sample() < par['cat_error_rate']:
            result = np.random.randint(0, nlevels)
        
        jsonDict[key] = labels[result]

    jsonString = json.dumps(jsonDict)
    return jsonString


class KafkaConsumer(Greenlet):
    
    def __init__(self):
        Greenlet.__init__(self)
        self.consumer = Consumer(config)
        smsg = ">> consumer connected <<"
        print(smsg)
        socketio.emit('newMessage', {'msg': smsg}, namespace='/epd')           
        self.runflag = True
        gevent.sleep()

    def run(self):
        global producer_total, consumer_total, consumer_topics, consumer_timeout
        self.consumer.subscribe(consumer_topics) 
        msg = None
        while self.runflag:
            try:
                msg = self.consumer.poll(timeout=consumer_timeout)
                if msg is None:
                    raise AttributeError
                if msg.error():
                    raise KafkaException(msg.error()) 
                consumer_total += 1
                value = msg.value().decode('utf-8')              
                socketio.emit('newMessage', {'msg': value}, namespace='/consumer')
                print ("-> consumer recv: ", value)
                smsg = "EPD Simulator Totals --> Produced: {}   Consumed: {}".format(producer_total, consumer_total)
                socketio.emit('newMessage', {'msg': smsg}, namespace='/epd')
                #print(msg.key().decode('utf-8'), msg.value().decode('utf-8'))
                gevent.sleep(0)
            except KafkaException:
                gevent.sleep(0.5)
                if msg.error().code() != KafkaError._PARTITION_EOF:
                    print("KafkaException: ", msg.error())
                # else:
                    # print("KafkaException: ", msg.error())
            except AttributeError:
                gevent.sleep(0.5)
                # print("Attribute Error: Nonetype object")

class KafkaProducer(Greenlet):
    
    def __init__(self, params):
        Greenlet.__init__(self)
        self.params = params
        self.producer = Producer(config)      
        self.runflag = True
        self.index = 0
        gevent.sleep()
        atexit.register(self.cleanup)

    def run(self):
        global producer_total, consumer_total, producer_topic
        # thread is stopped by setting close event
        while self.runflag:
            self.index += 1
            producer_total += 1
            val = makeRecord(self.params)
            key = str(self.index % 10)
            self.producer.produce(producer_topic, key=key.encode('utf-8'), value=val.encode('utf-8'))
            socketio.emit('newMessage', {'msg': val}, namespace='/producer')
            print("-> producer sent: ", val)
            smsg = "EPD Simulator Totals --> Produced: {}   Consumed: {}".format(producer_total, consumer_total)
            socketio.emit('newMessage', {'msg': smsg}, namespace='/epd')
            # add some random time delay between sent messages 
            delay = 0.5 + 2 * random.random()
            gevent.sleep(delay)

    def cleanup(self):
        try:
            self.producer.flush(timeout=0.5)
            self.producer.close(timeout=0.5)
        except:
            print("-> producer not initialized...")
 

@app.route('/')
def index():
    #only by sending this page first will the client be connected to the socketio instance
    return render_template('index.html')
  
@socketio.on('connect', namespace='/producer')
def producer_connect():
    print('>> producer socketIO connected')


@socketio.on('connect', namespace='/consumer')
def consumer_connect():
    print('>> consumer socketIO connected')


@socketio.on('connect', namespace='/epd')
def epd_connect():
    gevent.signal(signal.SIGQUIT, gevent.kill)
    print('>> epd socketIO connected')
    
        
@socketio.on('disconnect', namespace='/producer')
def producer_disconnect():
    print('>> producer socketIO disconnected')


@socketio.on('disconnect', namespace='/consumer')
def consumer_disconnect():
    print('>> consumer socketIO disconnected')


@socketio.on('disconnect', namespace='/epd')
def epd_disconnect():
    print('>> epd socketIO disconnected')
    
 
@socketio.on('message', namespace='/producer')
def producer_message(msg):  
    # if message is stop
    if msg == 'stop':
        if 'producer' in gthreads:
            p = gthreads['producer']
            p.runflag = False
            p.join()
            p.kill()
            del gthreads['producer']
    # start/stop producer thread if msg condition is met
    if msg == 'start':
        if 'producer' in gthreads:
            p = gthreads['producer']
            if not p.runflag:
                p.join()
                p.kill()
                del gthreads['producer']
                p = KafkaProducer(readParams())
                gthreads['producer'] = p
                p.start() 
        else:
            p = KafkaProducer(readParams())
            gthreads['producer'] = p
            p.start()
    

@socketio.on('message', namespace='/consumer')
def consumer_message(msg):  
    # if message is stop
    if msg == 'stop':
        if 'consumer' in gthreads:
            c = gthreads['consumer']
            c.runflag = False
            c.join()
            c.kill()
            del gthreads['consumer']
    # start/stop producer thread if msg condition is met
    if msg == 'start':           
        if 'consumer' in gthreads:
            c = gthreads['consumer']
            if not c.runflag:
                c.join()
                c.kill()
                del gthreads['consumer']
                c = KafkaConsumer()
                gthreads['consumer'] = c
                c.start()    
        else:
            c = KafkaConsumer()
            gthreads['consumer'] = c
            c.start() 
    

@socketio.on('broker', namespace='/epd')
def broker_update(msg):
    global config, consumer_timeout, producer_topic, consumer_topics
    params = json.loads(msg)
    producer_topic = params['producer_topic']
    consumer_topics = [params['consumer_topic']]
    consumer_timeout = float(params['consumer_timeout']) / 1000
    config['bootstrap.servers'] = params['bootstrap_server']
    print(config, producer_topic, consumer_topics, consumer_timeout)
    

# if __name__ == '__main__':
#     socketio.run(app)
